{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fnil\fcharset0 Roboto-Regular;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
{\info
{\author Jan-Hendrik Ruettinger}}\margl1440\margr1440\vieww10880\viewh8280\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
\
\
\
\
\
Week 1\
- Intro to competition & Recap of ML algos\
http://fastml.com/what-is-better-gradient-boosted-trees-or-random-forest/\
\
\
4 Classes of ML Algos\
Linear (Logistic Regression, SVM) => Sparse High dimensional data, scikit\
=> split space into 2 subspaces\
Tree-based (Decision Tree, Random Forest, GBDT) => Tabular data, hard to capture linear data, scikit\
=> split space into boxes\
kNN, scikit \
=> \
Neural Networks (Images, Sound, Text), Keras, Pytorch\
=> smooth non-linear decision boundary\
\
Most powerful methods: \
Gradient Boosted Decision Trees and Neural Networks\
\
No Free Lunch Theorem\
\'93There is no method which outperforms all others for all tasks\'94\
\
\
\
- Feature preprocessing & extraction\
https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/\
https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering\
http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\
- Feature preprocessing and feature generation muss stark an das Model angepasst sein\
\
Categorical features\
- label encoding: Maps categories to numbers\
- frequency encoding: Maps categories to their frequencies\
- One-hot encoding: Often used for non-tree-based methods\
- Combine features can often help linear models and KNN\
\
Tree-bases models\
- don\'92t depend on feature scaling\
- week 6 nur in test set => week 5\
 \
and Non-tree-based models\
- depend on feature scaling\
- feature encoding\
- one hot encoding\
\
Preprocessing: scaling\
sklearn.preprocessing.MinMaxScaler [0,1]\
Beispiel: Titanic Data set, Age and SibSp\
sklearn.preprocessing.StandardScaler mean=0, std=1\
\
Preprocessing: outliers\
\
pd.Series(x).hist(bins=30)\
\
Upperbound, Lowerbound = np.percentile(x, [1,99])\
y = np.clip(x, Upperbound, Lowerbound)\
pd.Series(x).hist(bins=30)\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 Preprocessing: rank\
=> F\'fcr linear models und NNs\
=> \
\
Feature generation\
=> Kreativit\'e4t gefragt\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 Type of features:\
- numeric\
- categorial\
- ordinal (geordnete kategorische Features)\
- Date and time (periodicity, time since: time till next holiday, difference between dates)\
- coordinates (Interesting places like distance to next hospital)\
\
\
\
Handling missing values\
Beispiel Springfield competition\
\
\
Text Features\
a. Preprocessing\
Lowercase. stemming, lemmatization, stop words\
b. Bag of words\
Huge vectors, Ngrams\
c. Word2vec\
Smaller vectors, pretrained models, King-men+women = queen\
\
\
Image Features\
- Finetuning = Vortrainiertes Netz trainieren\
- Augmentation (k\'fcnstlich neue Daten erstellen)\
\
\
\
Week 2: basic pipeline \
- EDA\
\
## Explore individual features\
- histogram\
- plot\
- statistics\
\
## Explore feature relations\
- Pairs\
	- Scatter plot, scatter matrix\
	- Corrplot\
- Groups\
	- Corrplot + clustering\
	- Plot (indes vs feature statistics)\
\
# Statistics\
- df.types\
- df.info()\
- x.value_counts()\
- x.isnull()\
- x.mean()\
\
# Histogram\
plt.hist(x)\
\
# Plots\
plt.scatter(range(len(x)), x, c=y) => Exploring feature relations\
pd.scatter_matrix(df)\
plt.plot(x, \'91.\'92)\
\
df.corr(), plt.matshow()\
\
df.mean().plot(style=\'91.\'92)\
df.mean().sort_vaues().plot(style=\'91.\'92)\
\
- Validation\
Number of splits\
Train/test splits\
Validation splits\
\
Underfitting refers to not capturing enough pattern in the data\
Overfitting refers to capturing noise/patterns which do not generalize to test data\
\
# Strategies\
- Holdout (Fold 1)\
=> sklearn.model_selection.Shufflesplit() \
- K-Fold Fold n)\
=> sklearn.model_selection.Kfold\
=> Fold n times and average performance scores\
=> n times slower than Holdout\
- Leave-one-out\
=> Ein einziges Datum als Validation set (wird kaum benutzt)\
\
=> Stratification: preserves the same target distribution over different folds\
\
- Data leaks\
\
Week 3: improve model\
- Metrics\
- Mean-encodings\
\
Week 4: improve model\
- Advanced features\
- Hyperparameter optimization\
- Ensembles\
\
Week 5: finalize your solution\
Final project\
Winning solutions}