{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fnil\fcharset0 Roboto-Regular;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
{\info
{\author Jan-Hendrik Ruettinger}}\margl1440\margr1440\vieww10880\viewh9600\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 0. Einf\'fchrung Kaggle\
\
\
1. Einf\'fchrung Pandas\
\
2. Einf\'fchrung Linear Regression \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 3. Einf\'fchrung Lernkurven (Slide)\
- Split data train/validation/test\
- Overfitting vs Underfitting\
- Regularization\
\
4. Titanic Challenge (\'fcben)\
- Data splitten, Learnkurve plotten\
\
5. Einf\'fchrung Scikit-Learn & Algos (Notebook)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 - Linear Regression (Regression)\
- Logistic Regression\
- SMV\
- Decision Tree/Random Forest\
- Gradient Boosting\
- GBDT(http://xgboost.readthedocs.io/en/latest/model.html  \
	 http://fastml.com/what-is-better-gradient-boosted-trees-or-random-forest/)\
- Neuronales Netz\
\
6. Feature engineering (Notebook)\
7. Kaggle challenge Go\
8. Ergebnisse pr\'e4nsentieren\
}